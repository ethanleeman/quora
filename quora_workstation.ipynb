{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "from sklearn.metrics import jaccard_score, log_loss\n",
    "import re\n",
    "import pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Leeman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "train = train.dropna()\n",
    "test = test.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unidecode.unidecode('\\'')\n",
    "from pattern.en import spelling\n",
    "\n",
    "word = \"amazzziiing\"\n",
    "word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "correct_word = spelling(word_wlf) \n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    #remove html/things in boxes\n",
    "    text = re.sub(\"\\[math\\].*\\[\\/math\\]\" , ' ', text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = str.lower(text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    text = re.sub(\"\\(.*\\)\", \" \", text)\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(\"[\\?\\#\\'\\\",.!:]+\", \"\", text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words] \n",
    "  \n",
    "    return tokens\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train['question1_cleaned'] = train['question1'].apply(clean)\n",
    "train['question2_cleaned'] = train['question2'].apply(clean)\n",
    "test['question1_cleaned'] = test['question1'].apply(clean)\n",
    "test['question2_cleaned'] = test['question2'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_cleaned.csv')\n",
    "test.to_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[story, kohinoor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[increase, speed, internet, connection, using,...</td>\n",
       "      <td>[internet, speed, increased, hacking, dns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mentally, lonely, solve]</td>\n",
       "      <td>[find, remainder, divided, 2423]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, dissolve, water, quikly, sugar, salt, me...</td>\n",
       "      <td>[fish, would, survive, salt, water]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  [step, step, guide, invest, share, market, india]   \n",
       "1                         [story, kohinoor, diamond]   \n",
       "2  [increase, speed, internet, connection, using,...   \n",
       "3                          [mentally, lonely, solve]   \n",
       "4  [one, dissolve, water, quikly, sugar, salt, me...   \n",
       "\n",
       "                                   question2_cleaned  \n",
       "0         [step, step, guide, invest, share, market]  \n",
       "1  [would, happen, indian, government, stole, koh...  \n",
       "2         [internet, speed, increased, hacking, dns]  \n",
       "3                   [find, remainder, divided, 2423]  \n",
       "4                [fish, would, survive, salt, water]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in share market in india?</td>\n",
       "      <td>What is the step by step guide to invest in share market?</td>\n",
       "      <td>0</td>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>\n",
       "      <td>What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?</td>\n",
       "      <td>0</td>\n",
       "      <td>[story, kohinoor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, kohinoor, diamond, back]</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet connection while using a VPN?</td>\n",
       "      <td>How can Internet speed be increased by hacking through DNS?</td>\n",
       "      <td>0</td>\n",
       "      <td>[increase, speed, internet, connection, using, vpn]</td>\n",
       "      <td>[internet, speed, increased, hacking, dns]</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve it?</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] is divided by 24,23?</td>\n",
       "      <td>0</td>\n",
       "      <td>[mentally, lonely, solve]</td>\n",
       "      <td>[find, remainder, divided, 2423]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, dissolve, water, quikly, sugar, salt, methane, carbon, di, oxide]</td>\n",
       "      <td>[fish, would, survive, salt, water]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2  \\\n",
       "0   0     1     2   \n",
       "1   1     3     4   \n",
       "2   2     5     6   \n",
       "3   3     7     8   \n",
       "4   4     9    10   \n",
       "\n",
       "                                                                      question1  \\\n",
       "0            What is the step by step guide to invest in share market in india?   \n",
       "1                           What is the story of Kohinoor (Koh-i-Noor) Diamond?   \n",
       "2     How can I increase the speed of my internet connection while using a VPN?   \n",
       "3                            Why am I mentally very lonely? How can I solve it?   \n",
       "4  Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?   \n",
       "\n",
       "                                                                                  question2  \\\n",
       "0                                 What is the step by step guide to invest in share market?   \n",
       "1  What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?   \n",
       "2                               How can Internet speed be increased by hacking through DNS?   \n",
       "3                         Find the remainder when [math]23^{24}[/math] is divided by 24,23?   \n",
       "4                                                   Which fish would survive in salt water?   \n",
       "\n",
       "   is_duplicate  \\\n",
       "0             0   \n",
       "1             0   \n",
       "2             0   \n",
       "3             0   \n",
       "4             0   \n",
       "\n",
       "                                                         question1_cleaned  \\\n",
       "0                        [step, step, guide, invest, share, market, india]   \n",
       "1                                               [story, kohinoor, diamond]   \n",
       "2                      [increase, speed, internet, connection, using, vpn]   \n",
       "3                                                [mentally, lonely, solve]   \n",
       "4  [one, dissolve, water, quikly, sugar, salt, methane, carbon, di, oxide]   \n",
       "\n",
       "                                                     question2_cleaned  \\\n",
       "0                           [step, step, guide, invest, share, market]   \n",
       "1  [would, happen, indian, government, stole, kohinoor, diamond, back]   \n",
       "2                           [internet, speed, increased, hacking, dns]   \n",
       "3                                     [find, remainder, divided, 2423]   \n",
       "4                                  [fish, would, survive, salt, water]   \n",
       "\n",
       "    jaccard  \n",
       "0  0.916667  \n",
       "1  0.285714  \n",
       "2  0.200000  \n",
       "3  0.000000  \n",
       "4  0.250000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(row):\n",
    "    s1 = set(row['question1_cleaned'])\n",
    "    s2 = set(row['question2_cleaned'])\n",
    "    u = set(s1 | s2)\n",
    "    i = set(s1 & s2)\n",
    "    if len(u) > 0:\n",
    "        return len(i)/len(u)\n",
    "    return 0\n",
    "train['jaccard'] = train.apply(jaccard,axis=1)\n",
    "test['jaccard'] = test.apply(jaccard,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Compute ROC curve and ROC area for each class\n",
    "y = np.array(train['is_duplicate'])\n",
    "scores = np.array(train['jaccard'])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=1)\n",
    "roc_auc = roc_auc_score(y, scores)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "y = np.array(train['is_duplicate'])\n",
    "x = np.array(train['jaccard']).reshape(-1,1)\n",
    "my_model = LogisticRegression(random_state=0,fit_intercept=True).fit(x, y)\n",
    "y_pred = my_model.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5768661603196288"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test['jaccard']).reshape(-1,1)\n",
    "test['is_duplicate'] = my_model.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare with iPad Pro?</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?</td>\n",
       "      <td>[surface, pro, 4, compare, ipad, pro]</td>\n",
       "      <td>[microsoft, choose, core, m3, core, i3, home, surface, pro, 4]</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.273146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How much would it cost?</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>[hair, transplant, age, 24, much, would, cost]</td>\n",
       "      <td>[much, cost, hair, transplant, require]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.445043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from China to the US?</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>[best, way, send, money, china, us]</td>\n",
       "      <td>[send, money, china]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.445043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>[food, emulsifiers]</td>\n",
       "      <td>[foods, fibre]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>[aberystwyth, start, reading]</td>\n",
       "      <td>[start, reading]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>How effective is there a need for a governor in Indian states? Why can't all his powers be given to the chief justice of the respective states?</td>\n",
       "      <td>Why is there a governor and chief minister to state in india?</td>\n",
       "      <td>[effective, need, governor, indian, states, powers, given, chief, justice, respective, states]</td>\n",
       "      <td>[governor, chief, minister, state, india]</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.201779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>When someone uses the \"devil horns\" hate gesture, what does it say about them?</td>\n",
       "      <td>How do improve add swinging hand gesture to my Android game?</td>\n",
       "      <td>[someone, uses, devil, horns, hate, gesture, say]</td>\n",
       "      <td>[improve, add, swinging, hand, gesture, android, game]</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.163589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>What are the wife vegetables for weight loss?</td>\n",
       "      <td>What are the best best vegetables for weight loss?</td>\n",
       "      <td>[wife, vegetables, weight, loss]</td>\n",
       "      <td>[best, best, vegetables, weight, loss]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.528171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>Why do I stammer? Is there any cure?</td>\n",
       "      <td>Is there any cure of stammering?</td>\n",
       "      <td>[stammer, cure]</td>\n",
       "      <td>[cure, stammering]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>Contemporary Art: What are some of companies best street art photos?</td>\n",
       "      <td>What is the best name to done to an Art Club in school?</td>\n",
       "      <td>[contemporary, art, companies, best, street, art, photos]</td>\n",
       "      <td>[best, name, done, art, club, school]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.227712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_id  \\\n",
       "0          0   \n",
       "1          1   \n",
       "2          2   \n",
       "3          3   \n",
       "4          4   \n",
       "..       ...   \n",
       "195      195   \n",
       "196      196   \n",
       "197      197   \n",
       "198      198   \n",
       "199      199   \n",
       "\n",
       "                                                                                                                                           question1  \\\n",
       "0                                                                                          How does the Surface Pro himself 4 compare with iPad Pro?   \n",
       "1                                                                                 Should I have a hair transplant at age 24? How much would it cost?   \n",
       "2                                                                                       What but is the best way to send money from China to the US?   \n",
       "3                                                                                                                        Which food not emulsifiers?   \n",
       "4                                                                                                                   How \"aberystwyth\" start reading?   \n",
       "..                                                                                                                                               ...   \n",
       "195  How effective is there a need for a governor in Indian states? Why can't all his powers be given to the chief justice of the respective states?   \n",
       "196                                                                   When someone uses the \"devil horns\" hate gesture, what does it say about them?   \n",
       "197                                                                                                    What are the wife vegetables for weight loss?   \n",
       "198                                                                                                             Why do I stammer? Is there any cure?   \n",
       "199                                                                             Contemporary Art: What are some of companies best street art photos?   \n",
       "\n",
       "                                                                question2  \\\n",
       "0    Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?   \n",
       "1                             How much cost does hair transplant require?   \n",
       "2                                           What you send money to China?   \n",
       "3                                                       What foods fibre?   \n",
       "4                                          How their can I start reading?   \n",
       "..                                                                    ...   \n",
       "195         Why is there a governor and chief minister to state in india?   \n",
       "196          How do improve add swinging hand gesture to my Android game?   \n",
       "197                    What are the best best vegetables for weight loss?   \n",
       "198                                      Is there any cure of stammering?   \n",
       "199               What is the best name to done to an Art Club in school?   \n",
       "\n",
       "                                                                                  question1_cleaned  \\\n",
       "0                                                             [surface, pro, 4, compare, ipad, pro]   \n",
       "1                                                    [hair, transplant, age, 24, much, would, cost]   \n",
       "2                                                               [best, way, send, money, china, us]   \n",
       "3                                                                               [food, emulsifiers]   \n",
       "4                                                                     [aberystwyth, start, reading]   \n",
       "..                                                                                              ...   \n",
       "195  [effective, need, governor, indian, states, powers, given, chief, justice, respective, states]   \n",
       "196                                               [someone, uses, devil, horns, hate, gesture, say]   \n",
       "197                                                                [wife, vegetables, weight, loss]   \n",
       "198                                                                                 [stammer, cure]   \n",
       "199                                       [contemporary, art, companies, best, street, art, photos]   \n",
       "\n",
       "                                                  question2_cleaned   jaccard  \\\n",
       "0    [microsoft, choose, core, m3, core, i3, home, surface, pro, 4]  0.272727   \n",
       "1                           [much, cost, hair, transplant, require]  0.500000   \n",
       "2                                              [send, money, china]  0.500000   \n",
       "3                                                    [foods, fibre]  0.000000   \n",
       "4                                                  [start, reading]  0.666667   \n",
       "..                                                              ...       ...   \n",
       "195                       [governor, chief, minister, state, india]  0.153846   \n",
       "196          [improve, add, swinging, hand, gesture, android, game]  0.076923   \n",
       "197                          [best, best, vegetables, weight, loss]  0.600000   \n",
       "198                                              [cure, stammering]  0.333333   \n",
       "199                           [best, name, done, art, club, school]  0.200000   \n",
       "\n",
       "     is_duplicate  \n",
       "0        0.273146  \n",
       "1        0.445043  \n",
       "2        0.445043  \n",
       "3        0.131436  \n",
       "4        0.583011  \n",
       "..            ...  \n",
       "195      0.201779  \n",
       "196      0.163589  \n",
       "197      0.528171  \n",
       "198      0.315056  \n",
       "199      0.227712  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['test_id','is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()\n",
    "submission.to_csv('my_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2Vec - take sums and take cosine similarity\n",
    "import gensim\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./../models/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(row):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for w in row['question1_cleaned']:\n",
    "        if model.__contains__(w):\n",
    "            s1.append(w)\n",
    "    for w in row['question2_cleaned']:\n",
    "        if model.__contains__(w):\n",
    "            s2.append(w)\n",
    "    \n",
    "    \n",
    "    if len(s1) > 0 and len(s2) > 0:\n",
    "        return model.n_similarity(s1,s2)\n",
    "    return 0          \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                   0\n",
       "qid1                                                                 1\n",
       "qid2                                                                 2\n",
       "question1            What is the step by step guide to invest in sh...\n",
       "question2            What is the step by step guide to invest in sh...\n",
       "is_duplicate                                                         0\n",
       "question1_cleaned    [step, step, guide, invest, share, market, india]\n",
       "question2_cleaned           [step, step, guide, invest, share, market]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48783562"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['google_w2v_similarity'] = train.apply(similarity,axis=1)\n",
    "test['google_w2v_similarity'] = test.apply(similarity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>google_w2v_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404287.000000</td>\n",
       "      <td>404287.000000</td>\n",
       "      <td>404287.000000</td>\n",
       "      <td>404287.000000</td>\n",
       "      <td>404287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202144.340337</td>\n",
       "      <td>217243.151093</td>\n",
       "      <td>220955.212082</td>\n",
       "      <td>0.369201</td>\n",
       "      <td>0.738924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116708.673691</td>\n",
       "      <td>157751.614317</td>\n",
       "      <td>159903.168488</td>\n",
       "      <td>0.482589</td>\n",
       "      <td>0.214072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101071.500000</td>\n",
       "      <td>74436.500000</td>\n",
       "      <td>74726.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>202145.000000</td>\n",
       "      <td>192181.000000</td>\n",
       "      <td>197053.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>303216.500000</td>\n",
       "      <td>346573.000000</td>\n",
       "      <td>354692.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>404289.000000</td>\n",
       "      <td>537932.000000</td>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate  \\\n",
       "count  404287.000000  404287.000000  404287.000000  404287.000000   \n",
       "mean   202144.340337  217243.151093  220955.212082       0.369201   \n",
       "std    116708.673691  157751.614317  159903.168488       0.482589   \n",
       "min         0.000000       1.000000       2.000000       0.000000   \n",
       "25%    101071.500000   74436.500000   74726.500000       0.000000   \n",
       "50%    202145.000000  192181.000000  197053.000000       0.000000   \n",
       "75%    303216.500000  346573.000000  354692.000000       1.000000   \n",
       "max    404289.000000  537932.000000  537933.000000       1.000000   \n",
       "\n",
       "       google_w2v_similarity  \n",
       "count          404287.000000  \n",
       "mean                0.738924  \n",
       "std                 0.214072  \n",
       "min                -0.120409  \n",
       "25%                 0.643393  \n",
       "50%                 0.788785  \n",
       "75%                 0.895114  \n",
       "max                 1.000000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Word2VecKeyedVectors(vector_size)\n",
      " |  \n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x1a2d0620e0>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train['google_w2v_similarity'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>['step', 'step', 'guide', 'invest', 'share', '...</td>\n",
       "      <td>['step', 'step', 'guide', 'invest', 'share', '...</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>['story', 'kohinoor', 'diamond']</td>\n",
       "      <td>['would', 'happen', 'indian', 'government', 's...</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>['increase', 'speed', 'internet', 'connection'...</td>\n",
       "      <td>['internet', 'speed', 'increased', 'hacking', ...</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>['mentally', 'lonely', 'solve']</td>\n",
       "      <td>['find', 'remainder', 'divided', '2423']</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>['one', 'dissolve', 'water', 'quikly', 'sugar'...</td>\n",
       "      <td>['fish', 'would', 'survive', 'salt', 'water']</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           0   0     1     2   \n",
       "1           1   1     3     4   \n",
       "2           2   2     5     6   \n",
       "3           3   3     7     8   \n",
       "4           4   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  ['step', 'step', 'guide', 'invest', 'share', '...   \n",
       "1                   ['story', 'kohinoor', 'diamond']   \n",
       "2  ['increase', 'speed', 'internet', 'connection'...   \n",
       "3                    ['mentally', 'lonely', 'solve']   \n",
       "4  ['one', 'dissolve', 'water', 'quikly', 'sugar'...   \n",
       "\n",
       "                                   question2_cleaned   jaccard  \n",
       "0  ['step', 'step', 'guide', 'invest', 'share', '...  0.833333  \n",
       "1  ['would', 'happen', 'indian', 'government', 's...  0.222222  \n",
       "2  ['internet', 'speed', 'increased', 'hacking', ...  0.222222  \n",
       "3           ['find', 'remainder', 'divided', '2423']  0.000000  \n",
       "4      ['fish', 'would', 'survive', 'salt', 'water']  0.153846  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>['step', 'step', 'guide', 'invest', 'share', '...</td>\n",
       "      <td>['step', 'step', 'guide', 'invest', 'share', '...</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>['story', 'kohinoor', 'diamond']</td>\n",
       "      <td>['would', 'happen', 'indian', 'government', 's...</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>['increase', 'speed', 'internet', 'connection'...</td>\n",
       "      <td>['internet', 'speed', 'increased', 'hacking', ...</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>['mentally', 'lonely', 'solve']</td>\n",
       "      <td>['find', 'remainder', 'divided', '2423']</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>['one', 'dissolve', 'water', 'quikly', 'sugar'...</td>\n",
       "      <td>['fish', 'would', 'survive', 'salt', 'water']</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           0   0     1     2   \n",
       "1           1   1     3     4   \n",
       "2           2   2     5     6   \n",
       "3           3   3     7     8   \n",
       "4           4   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  ['step', 'step', 'guide', 'invest', 'share', '...   \n",
       "1                   ['story', 'kohinoor', 'diamond']   \n",
       "2  ['increase', 'speed', 'internet', 'connection'...   \n",
       "3                    ['mentally', 'lonely', 'solve']   \n",
       "4  ['one', 'dissolve', 'water', 'quikly', 'sugar'...   \n",
       "\n",
       "                                   question2_cleaned   jaccard  \n",
       "0  ['step', 'step', 'guide', 'invest', 'share', '...  0.833333  \n",
       "1  ['would', 'happen', 'indian', 'government', 's...  0.222222  \n",
       "2  ['internet', 'speed', 'increased', 'hacking', ...  0.222222  \n",
       "3           ['find', 'remainder', 'divided', '2423']  0.000000  \n",
       "4      ['fish', 'would', 'survive', 'salt', 'water']  0.153846  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Compute ROC curve and ROC area for each class\n",
    "y = np.array(train['is_duplicate'])\n",
    "scores = np.array(train['google_w2v_similarity'])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=1)\n",
    "roc_auc = roc_auc_score(y, scores)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "y = np.array(train['is_duplicate'])\n",
    "x = np.array(train[['google_w2v_similarity']])\n",
    "my_model = LogisticRegression(random_state=0,fit_intercept=True).fit(x, y)\n",
    "y_pred = my_model.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5681543951929624"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test['google_w2v_similarity']).reshape(-1,1)\n",
    "test['is_duplicate'] = my_model.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>google_w2v_similarity</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "      <td>0.931028</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[story, kohinoor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "      <td>0.487836</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[increase, speed, internet, connection, using,...</td>\n",
       "      <td>[internet, speed, increased, hacking, dns]</td>\n",
       "      <td>0.777991</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mentally, lonely, solve]</td>\n",
       "      <td>[find, remainder, divided, 2423]</td>\n",
       "      <td>0.278192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, dissolve, water, quikly, sugar, salt, me...</td>\n",
       "      <td>[fish, would, survive, salt, water]</td>\n",
       "      <td>0.630007</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  [step, step, guide, invest, share, market, india]   \n",
       "1                         [story, kohinoor, diamond]   \n",
       "2  [increase, speed, internet, connection, using,...   \n",
       "3                          [mentally, lonely, solve]   \n",
       "4  [one, dissolve, water, quikly, sugar, salt, me...   \n",
       "\n",
       "                                   question2_cleaned  google_w2v_similarity  \\\n",
       "0         [step, step, guide, invest, share, market]               0.931028   \n",
       "1  [would, happen, indian, government, stole, koh...               0.487836   \n",
       "2         [internet, speed, increased, hacking, dns]               0.777991   \n",
       "3                   [find, remainder, divided, 2423]               0.278192   \n",
       "4                [fish, would, survive, salt, water]               0.630007   \n",
       "\n",
       "    jaccard  \n",
       "0  0.833333  \n",
       "1  0.222222  \n",
       "2  0.222222  \n",
       "3  0.000000  \n",
       "4  0.153846  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([143415.21414381])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.correlate(train['google_w2v_similarity'],train['jaccard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
